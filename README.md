# Airline-Passenger-Satisfaction-Prediction
This project focuses on predicting airline passenger satisfaction using a structured Machine Learning workflow. The dataset contains information about passenger demographics, flight experience, service ratings, and arrival delays. The objective is to classify passengers as either “satisfied” or “neutral or dissatisfied” based on multiple features, preparing the data for model training using a robust preprocessing pipeline.

The project begins with data exploration, where Python libraries such as Pandas, NumPy, Matplotlib, and Seaborn are used to inspect the dataset. Functions like head(), info(), and isnull().sum() help understand the structure, data types, missing values, and basic statistics. The target variable distribution is visualized with a countplot, highlighting class imbalance and guiding preprocessing decisions.

Next, the dataset is cleaned and prepared. Irrelevant columns, such as id, are removed. Features are separated into numerical and nominal categorical variables, while the target column (satisfaction) is treated as ordinal. Missing values in numerical features are handled using median imputation, and numerical features are scaled with StandardScaler. Categorical features are imputed using the most frequent value and encoded via OneHotEncoder, while the ordinal target is encoded using OrdinalEncoder with a defined category order.

To streamline preprocessing, pipelines are built for numerical, categorical, and ordinal features, then combined using ColumnTransformer. This ensures consistent and reproducible transformations across training and testing datasets. The data is split into training and testing sets using an 80/20 split with train_test_split, ensuring that models can be trained and evaluated fairly.

This project demonstrates a complete end-to-end ML preprocessing workflow, covering data cleaning, feature engineering, encoding, scaling, and pipeline automation. The output is a model-ready dataset, ready for training with algorithms such as Logistic Regression, Random Forest, SVM, XGBoost, or K-Nearest Neighbors. It highlights practical skills for internships, Kaggle projects, or real-world ML applications, emphasizing clean, scalable, and modular ML code.
